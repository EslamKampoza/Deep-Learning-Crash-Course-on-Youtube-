{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EslamKampoza/machine-intelligence-paper-/blob/main/oppo_SMOTE_DMLPCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtuNJ9meatPj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "#import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "#from tensorflow.keras import regularizers\n",
        "#from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, SimpleRNN, GRU, LSTM, GlobalMaxPooling1D,GlobalMaxPooling2D,MaxPooling2D,BatchNormalization\n",
        "#from tensorflow.keras.models import Model\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eGfS04SEaxYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7iybO6natPq"
      },
      "outputs": [],
      "source": [
        "class models():\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "       \n",
        "    \n",
        "    def read_h5(self):\n",
        "        f = h5py.File(path, 'r')\n",
        "        X = f.get('inputs')\n",
        "        y = f.get('labels') \n",
        "        #print(type(X))\n",
        "        #print(type(y))\n",
        "        self.X = np.array(X)\n",
        "        self.y = np.array(y)\n",
        "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.4, random_state = 1)\n",
        "    \n",
        "        print(\"X = \", self.X.shape)\n",
        "        print(\"y =\",self.y.shape)\n",
        "        print(set(self.y))\n",
        "        #return X,y\n",
        "    \n",
        "    def cnn_model(self):\n",
        "       # K = len(set(y_train))\n",
        "        #print(K)\n",
        "        K = len(set(self.y))\n",
        "        #X = np.expand_dims(X, -1)\n",
        "        self.x_train = np.expand_dims(self.x_train, -1)\n",
        "        self.x_test = np.expand_dims(self.x_test,-1)\n",
        "        #print(X)\n",
        "        #print(X[0].shape)\n",
        "        #i = Input(shape=X[0].shape)\n",
        "        i = Input(shape=self.x_train[0].shape)\n",
        "        x = Conv2D(32, (3,3), strides = 2, activation = 'relu',padding='same',kernel_regularizer=regularizers.l2(0.0005))(i)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D((2,2))(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Conv2D(64, (3,3), strides = 2, activation = 'relu',padding='same',kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "        x = Conv2D(128, (3,3), strides = 2, activation = 'relu',padding='same',kernel_regularizer=regularizers.l2(0.0005))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D((2,2))(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Flatten()(x)    \n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(1024,activation = 'relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(K, activation = 'softmax')(x)       \n",
        "        self.model = Model(i,x)\n",
        "        self.model.compile(optimizer = Adam(lr=0.001),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "        #self.r = model.fit(X, y, validation_split = 0.4, epochs = 50, batch_size = 32 )\n",
        "        self.r = self.model.fit(self.x_train, self.y_train, validation_data = (self.x_test, self.y_test), epochs = 50, batch_size = 32 )\n",
        "        print(self.model.summary())\n",
        "        # It is better than using keras do the splitting!!\n",
        "        return self.r\n",
        "    \n",
        "    def dnn_model(self):\n",
        "       # K = len(set(y_train))\n",
        "        #print(K)\n",
        "        K = len(set(self.y))\n",
        "        print(self.x_train[0].shape)\n",
        "        i = Input(shape=self.x_train[0].shape)\n",
        "        x = Flatten()(i)\n",
        "        x = Dense(128,activation = 'relu')(x)\n",
        "        x = Dense(128,activation = 'relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "        x = Dense(256,activation = 'relu')(x)\n",
        "        x = Dense(256,activation = 'relu')(x)\n",
        "        x = Dense(256,activation = 'relu')(x)\n",
        "        #x = Dropout(0.2)(x)\n",
        "        x = Dense(1024,activation = 'relu')(x)\n",
        "        x = Dense(K,activation = 'softmax')(x)\n",
        "        self.model = Model(i,x)      \n",
        "        self.model.compile(optimizer = Adam(lr=0.001),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "        \n",
        "        '''\n",
        "        K = len(set(self.y))\n",
        "        model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=self.x_train[0].shape),\n",
        "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(K,activation = 'softmax')\n",
        "        ])\n",
        "        model.compile(optimizer = Adam(lr=0.0005),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "        '''\n",
        "        self.r = self.model.fit(self.x_train, self.y_train, validation_data = (self.x_test, self.y_test), epochs = 50 )\n",
        "        print(self.model.summary())\n",
        "        return self.r\n",
        "    \n",
        "\n",
        "    def rnn_model(self):\n",
        "        K = len(set(self.y))\n",
        "        i = Input(shape = self.x_train[0].shape)\n",
        "        x = LSTM(256, return_sequences=True)(i)\n",
        "        x = Dense(128,activation = 'relu')(x)\n",
        "        x = GlobalMaxPooling1D()(x)\n",
        "        x = Dense(K,activation = 'softmax')(x)\n",
        "        self.model = Model(i,x)      \n",
        "        self.model.compile(optimizer = Adam(lr=0.001),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics = ['accuracy'])\n",
        "        self.r = self.model.fit(self.x_train, self.y_train, validation_data = (self.x_test, self.y_test), epochs = 50, batch_size = 32 )\n",
        "        #self.r = model.fit(X, y, validation_split = 0.2, epochs = 10, batch_size = 32 )\n",
        "        print(self.model.summary())\n",
        "        return self.r\n",
        "   \n",
        "    def draw(self):\n",
        "        f1 = plt.figure(1)\n",
        "        plt.title('Loss')\n",
        "        plt.plot(self.r.history['loss'], label = 'loss')\n",
        "        plt.plot(self.r.history['val_loss'], label = 'val_loss')\n",
        "        plt.legend()\n",
        "        f1.show()\n",
        "        \n",
        "        f2 = plt.figure(2)\n",
        "        plt.plot(self.r.history['acc'], label = 'accuracy')\n",
        "        plt.plot(self.r.history['val_acc'], label = 'val_accuracy')\n",
        "        plt.legend()\n",
        "        f2.show()\n",
        "        \n",
        "    # summary, confusion matrix and heatmap\n",
        "    def con_matrix(self):\n",
        "        K = len(set(self.y_train))\n",
        "        self.y_pred = self.model.predict(self.x_test).argmax(axis=1)\n",
        "        cm = confusion_matrix(self.y_test,self.y_pred)\n",
        "        self.plot_confusion_matrix(cm,list(range(K)))\n",
        "            \n",
        "    \n",
        "    def plot_confusion_matrix(self, cm, classes, normalize = False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
        "            print(\"Normalized confusion matrix\")\n",
        "        else:\n",
        "            print(\"Confusion matrix, without normalization\")\n",
        "        print(cm)\n",
        "        f3 = plt.figure(3)\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        plt.title(title)\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        plt.xticks(tick_marks, classes, rotation=45)\n",
        "        plt.yticks(tick_marks, classes)\n",
        "        \n",
        "        fmt = '.2f' if normalize else 'd'\n",
        "        thresh = cm.max()/2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, format(cm[i, j], fmt),\n",
        "                     horizontalalignment = \"center\",\n",
        "                     color = \"white\" if cm[i, j] > thresh else \"black\")\n",
        "            plt.tight_layout()\n",
        "            plt.ylabel('True label')\n",
        "            plt.xlabel('predicted label')\n",
        "            f3.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DHQYCZFatPt"
      },
      "outputs": [],
      "source": [
        "model_name = \"rnn\" # can be cnn/dnn/rnn\n",
        "loco = True # True is to use locomotion as labels. False is to use high level activities as labels\n",
        "#path = \"loco_2.h5\"\n",
        "path = \"\"\n",
        "if loco:\n",
        "    path = \"./drive/MyDrive/datasets/loco_2.h5\"\n",
        "else:\n",
        "    path = \"./drive/MyDrive/datasets/hl_2.h5\"\n",
        "        \n",
        "oppo = models(path)\n",
        "    \n",
        "print(\"read h5 file....\")\n",
        "oppo.read_h5()   \n",
        "#if model_name == \"cnn\":\n",
        "#    oppo.cnn_model()\n",
        "#elif model_name == \"dnn\":\n",
        "#     oppo.dnn_model()\n",
        "#elif model_name == \"rnn\":\n",
        "#     oppo.rnn_model()\n",
        "#oppo.draw()\n",
        "#oppo.con_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abjg6WXdatPv"
      },
      "outputs": [],
      "source": [
        "unique,counts = np.unique(oppo.y,return_counts = True)\n",
        "dict(zip(unique,counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q1oKUwUatPw"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZGwmVpPatPx"
      },
      "outputs": [],
      "source": [
        "##Ploting Confusion Matrix\n",
        "def plot_confusion_matrix (cm, classes, normalize=False, title='Confusion Matrix', cmap=pyplot.cm.Blues,\n",
        "                           decsnTreeClf=None):\n",
        "    #This function prints and plots the confusion matrix.\n",
        "    pyplot.imshow(cm,interpolation='nearest',cmap=cmap)\n",
        "    pyplot.title(title)\n",
        "    pyplot.colorbar()\n",
        "    tick_marks=np.arange(len(classes))\n",
        "    pyplot.xticks(tick_marks,classes,rotation=45)\n",
        "    pyplot.yticks(tick_marks,classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:,np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print(\"Confusion matrix, without normalization\")\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max()/2.\n",
        "    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        pyplot.text(j,i,cm[i,j],horizontalalignment=\"center\", color=\"white\" if cm[i,j]> thresh else \"black\" )\n",
        "\n",
        "    pyplot.tight_layout()\n",
        "    pyplot.ylabel('True Label')\n",
        "    pyplot.xlabel('Predicted Label')\n",
        "    pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ2C69cUatPy"
      },
      "outputs": [],
      "source": [
        "xshape = oppo.x_train.shape\n",
        "newx_train = oppo.x_train.reshape(xshape[0],(xshape[1]*xshape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehWKeosEatPz"
      },
      "outputs": [],
      "source": [
        "xshape = oppo.x_test.shape\n",
        "newx_test = oppo.x_test.reshape(xshape[0],(xshape[1]*xshape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca= PCA(0.9) # capture the %85 of the variance\n",
        "pca.fit(newx_train)\n",
        "newx_train=pca.transform(newx_train)\n",
        "newx_test=pca.transform(newx_test)"
      ],
      "metadata": {
        "id": "IJjBQPDkLHct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install metric-learn\n",
        "import metric_learn"
      ],
      "metadata": {
        "id": "UKxm83SXf1Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmc = metric_learn.ITML_Supervised()\n",
        "X_mmc = mmc.fit(newx_train, oppo.y_train)"
      ],
      "metadata": {
        "id": "Bz-KU_1Zfs2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newx_train.shape\n",
        "MM = mmc.get_mahalanobis_matrix()\n",
        "MM.shape"
      ],
      "metadata": {
        "id": "n2x_vPSCfzIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import math\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Smote(object):\n",
        "\t\"\"\"docstring for Smote\"\"\"\n",
        "\n",
        "\tdef __init__(self,distance,MM):\n",
        "\t\tsuper(Smote, self).__init__()\n",
        "\t\tself.synthetic_arr=  []\n",
        "\t\tself.newindex = 0\n",
        "\t\tself.distance_measure = distance\n",
        "\t\tself.mmc = MM       \n",
        "\n",
        "\n",
        "\n",
        "\tdef Populate(self,N,i,indices,min_samples,k):\n",
        "\t\t\"\"\"\n",
        "    \t\tPopulates the synthitic array\n",
        "\n",
        "\n",
        "    \t\tReturns:Synthetic Array to generate_syntheic_points \n",
        "    \t\"\"\"\n",
        "\n",
        "\t\twhile N!=0:\n",
        "\t\t\tarr = []\n",
        "\t\t\tnn = randint(0,k-2)\n",
        "\t\t\tfeatures = len(min_samples[0])\n",
        "\t\t\t\n",
        "\t\t\tfor attr in range(features):\n",
        "\t\t\t\tdiff = min_samples[indices[nn]][attr] - min_samples[i][attr]\n",
        "\t\t\t\tgap = random.uniform(0,1)\n",
        "\t\t\t\tarr.append(min_samples[i][attr] + gap*diff)\n",
        "\t\t\t\n",
        "\t\t\tself.synthetic_arr.append(arr)\n",
        "\t\t\tself.newindex = self.newindex + 1\n",
        "\t\t\tN = N-1\n",
        "\n",
        "\n",
        "\n",
        "\tdef k_neighbors(self,euclid_distance,k):\n",
        "\t\tnearest_idx_npy = np.empty([euclid_distance.shape[0],euclid_distance.shape[0]],dtype=np.int64)\n",
        "\t\t\n",
        "\t\tfor i in range(len(euclid_distance)):\n",
        "\t\t\tidx = np.argsort(euclid_distance[i])\n",
        "\t\t\tnearest_idx_npy[i] = idx\n",
        "\t\t\tidx = 0\n",
        "\n",
        "\t\treturn nearest_idx_npy[:,1:k]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef find_k(self,X,k):\n",
        "\n",
        "\t\t\"\"\"\n",
        "   \t\t\tFinds k nearest neighbors using euclidian distance\n",
        "\n",
        "   \t\t\tReturns: The k nearest neighbor   \n",
        "    \t\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\teuclid_distance = np.empty([X.shape[0],X.shape[0]],dtype = np.float32)\n",
        "\t\t\n",
        "\t\tfor i in range(len(X)):\n",
        "\t\t\tdist_arr = []\n",
        "\t\t\tfor j in range(len(X)):\n",
        "\t\t\t\tdist_arr.append(math.sqrt(sum((X[j]-X[i])**2)))\n",
        "\t\t\tdist_arr = np.asarray(dist_arr,dtype = np.float32)\n",
        "\t\t\teuclid_distance[i] = dist_arr\n",
        "\n",
        "\t\treturn self.k_neighbors(euclid_distance,k)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef generate_synthetic_points(self,min_samples,N,k):\n",
        "\n",
        "\t\t\"\"\"\n",
        "    \t\tReturns (N/100) * n_minority_samples synthetic minority samples.\n",
        "    \t\tParameters\n",
        "    \t\t----------\n",
        "    \t\tmin_samples : Numpy_array-like, shape = [n_minority_samples, n_features]\n",
        "    \t\t    Holds the minority samples\n",
        "    \t\tN : percetange of new synthetic samples: \n",
        "    \t\t    n_synthetic_samples = N/100 * n_minority_samples. Can be < 100.\n",
        "    \t\tk : int. Number of nearest neighbours. \n",
        "    \t\tReturns\n",
        "    \t\t-------\n",
        "    \t\tS : Synthetic samples. array, \n",
        "    \t\t    shape = [(N/100) * n_minority_samples, n_features]. \n",
        "    \t\"\"\"\n",
        "\t\t\n",
        "\n",
        "\t\tif N < 100:\n",
        "\t\t\traise ValueError(\"Value of N cannot be less than 100%\")\n",
        "\n",
        "\t\tif self.distance_measure not in ('euclidian','ball_tree','Mahal'):\n",
        "\t\t\traise ValueError(\"Invalid Distance Measure.You can use only Euclidian or ball_tree\")\n",
        "\n",
        "\n",
        "\t\tif k>min_samples.shape[0]:\n",
        "\t\t\traise ValueError(\"Size of k cannot exceed the number of samples.\")\n",
        "\n",
        "\t\t\n",
        "\t\tN = int(N/100)\n",
        "\t\tT = min_samples.shape[0]\n",
        "\t\t\n",
        "\t\t\n",
        "\n",
        "\t\tif self.distance_measure == 'euclidian':\n",
        "\t\t\t\tindices = self.find_k(min_samples,k)\n",
        "\t\t\t\n",
        "\t\telif self.distance_measure=='ball_tree':\n",
        "\t\t\tnb = NearestNeighbors(n_neighbors = k,algorithm= 'ball_tree').fit(min_samples)\n",
        "\t\t\tdistance,indices = nb.kneighbors(min_samples)\n",
        "\t\t\tindices = indices[:,1:]\t\n",
        "\t\telif self.distance_measure=='Mahal':\n",
        "\t\t\tnb = NearestNeighbors(n_neighbors = k,metric = mmc.get_metric()).fit(min_samples)\n",
        "\t\t\tdistance,indices = nb.kneighbors(min_samples)\n",
        "\t\t\tindices = indices[:,1:]\t\n",
        "\n",
        "\n",
        "\t\tfor i in range(indices.shape[0]):\n",
        "\t\t\tself.Populate(N,i,indices[i],min_samples,k)\n",
        "\t\t\n",
        "\t\treturn np.asarray(self.synthetic_arr)\n",
        "\n",
        "\t\t\n",
        "\n",
        "\n",
        "\tdef plot_synthetic_points(self,min_samples,N,k):\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t\tPlot the over sampled synthtic samples in a scatterplot\n",
        "\n",
        "\n",
        "\n",
        "\t\t\"\"\"\n",
        "\n",
        "\n",
        "\t\tif N < 100:\n",
        "\t\t\traise ValueError(\"Value of N cannot be less than 100%\")\n",
        "\n",
        "\t\tif self.distance_measure not in ('euclidian','ball_tree','Mahal'):\n",
        "\t\t\traise ValueError(\"Invalid Distance Measure.You can use only Euclidian or ball_tree\")\n",
        "\n",
        "\n",
        "\t\tif k>min_samples.shape[0]:\n",
        "\t\t\traise ValueError(\"Size of k cannot exceed the number of samples.\")\n",
        "\n",
        "\t\t\n",
        "\t\tsynthetic_points = self.generate_synthetic_points(min_samples,N,k)\n",
        "\t\t\n",
        "\t\tpca = PCA(n_components=2)\n",
        "\t\tpca.fit(synthetic_points)\n",
        "\t\tpca_synthetic_points = pca.transform(synthetic_points)\n",
        "\t\t\n",
        "\t\tplt.scatter(pca_synthetic_points[:,0],pca_synthetic_points[:,1])\n",
        "\t\tplt.show()\n",
        "\t\t"
      ],
      "metadata": {
        "id": "cQkBTLWrgZkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfr85bl0gdG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainLabel = oppo.y_train.reshape(len(oppo.y_train),1)\n",
        "train = np.hstack((newx_train, trainLabel))"
      ],
      "metadata": {
        "id": "HVU0hsPeK3H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qUmDaAbZKbVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique,counts = np.unique(trainLabel,return_counts = True)\n",
        "dict(zip(unique,counts))"
      ],
      "metadata": {
        "id": "KY8BrhoPhk2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = train[:,142] == 0\n",
        "class0 = train[mask]\n",
        "class0 = class0[:,:142]\n",
        "N = (8356-1041) * 100/1041\n",
        "k = 12\n",
        "class0 = np.float_(class0)\n",
        "smote_test1 = Smote('Mahal',mmc)\n",
        "new_class0 = smote_test1.generate_synthetic_points(class0[:,:142],N,k)\n",
        "\n",
        "myclass = np.array([0 for x in range(len(new_class0))])\n",
        "#myclass = [\"Standing\" for x in range(1500)]\n",
        "myclass = myclass.reshape(len(new_class0),1)\n",
        "new_class0 = np.hstack((new_class0[:,:142],myclass))"
      ],
      "metadata": {
        "id": "NnfUDAdjgr2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = train[:,142] == 2\n",
        "class2 = train[mask]\n",
        "class2 = class0[:,:142]\n",
        "N = 4445*100/8356+100\n",
        "#(8356-4445) * 100/4445\n",
        "k = 12\n",
        "class2 = np.float_(class2)\n",
        "smote_test1 = Smote('Mahal',mmc)\n",
        "new_class2 = smote_test1.generate_synthetic_points(class2[:,:142],N,k)\n",
        "\n",
        "myclass = np.array([2 for x in range(len(new_class2))])\n",
        "#myclass = [\"Standing\" for x in range(1500)]\n",
        "myclass = myclass.reshape(len(new_class2),1)\n",
        "new_class2 = np.hstack((new_class2[:,:142],myclass))"
      ],
      "metadata": {
        "id": "ldpXv1DLk-u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = train[:,142] == 3\n",
        "class3 = train[mask]\n",
        "class3 = class0[:,:142]\n",
        "N = 4783*100/8356+100\n",
        "#(8356-4445) * 100/4445\n",
        "k = 12\n",
        "class3 = np.float_(class3)\n",
        "smote_test1 = Smote('Mahal',mmc)\n",
        "new_class3 = smote_test1.generate_synthetic_points(class3[:,:142],N,k)\n",
        "\n",
        "myclass = np.array([3 for x in range(len(new_class3))])\n",
        "#myclass = [\"Standing\" for x in range(1500)]\n",
        "myclass = myclass.reshape(len(new_class3),1)\n",
        "new_class3 = np.hstack((new_class3[:,:142],myclass))"
      ],
      "metadata": {
        "id": "ed3Aj3o7lj6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_data = []"
      ],
      "metadata": {
        "id": "mK45uJLbrPDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_data = np.concatenate((newx_train, new_class0[:,:142]), axis=0)\n",
        "new_train_data = np.concatenate((new_train_data, new_class2[:,:142]), axis=0)\n",
        "new_train_data = np.concatenate((new_train_data, new_class3[:,:142]), axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "8PyFZPiTm4wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_labels= []"
      ],
      "metadata": {
        "id": "8CKxOm7CrZBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "new_train_labels = np.concatenate((trainLabel, new_class0[:,-1].reshape(len(new_class0[:,-1]),1)), axis=0)\n",
        "new_train_labels = np.concatenate((new_train_labels, new_class2[:,-1].reshape(len(new_class2[:,-1]),1)), axis=0)\n",
        "new_train_labels = np.concatenate((new_train_labels, new_class3[:,-1].reshape(len(new_class3[:,-1]),1)), axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "qmtCKPQhoztR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "new_train_labels = new_train_labels.reshape(len(new_train_labels),1)"
      ],
      "metadata": {
        "id": "o0yL8QZrrwQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique,counts = np.unique(new_train_labels,return_counts = True)\n",
        "dict(zip(unique,counts))"
      ],
      "metadata": {
        "id": "V4Kl_orlfKGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, StandardScaler , MinMaxScaler\n",
        "#scaler = MinMaxScaler()\n",
        "#scaler.fit(new_train_data)\n",
        "#trainData = scaler.transform(new_train_data)\n",
        "#testData = scaler.transform(newx_test)"
      ],
      "metadata": {
        "id": "lcl6Y9o6OIHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_res = new_train_data\n",
        "labels_res = new_train_labels\n",
        "testLabelE = oppo.y_test"
      ],
      "metadata": {
        "id": "vQs5yB7Kr-pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding Labels\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "#encoding test labels\n",
        "\n",
        "encoder.fit(oppo.y_test)\n",
        "testLabelE = encoder.transform(oppo.y_test)\n",
        "\n",
        "#encoding train labels\n",
        "\n",
        "encoder.fit(labels_res)\n",
        "trainLabelE = encoder.transform(labels_res)"
      ],
      "metadata": {
        "id": "pedSRKq9N_Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5OEic48atP4",
        "outputId": "dcbe53ac-f4f2-4f5c-81d6-d8802e2748eb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "clf2= KNeighborsClassifier(n_neighbors=4, metric = mmc.get_metric())\n",
        "knnModel2 = clf2.fit(Data_res,labels_res)\n",
        "y_te_pred2 = clf2.predict(newx_test)\n",
        "\n",
        "acc2 = accuracy_score(testLabelE,y_te_pred2)\n",
        "print(\"K-Nearest Neighbors Accuracy: %.5f\" %(acc2)) #.907\n",
        "cfs=confusion_matrix(testLabelE,y_te_pred2)\n",
        "pyplot.figure()\n",
        "class_names = encoder.classes_\n",
        "plot_confusion_matrix(cfs,classes=class_names,title=\"Original: Confusuion Matrix\")\n",
        "print(classification_report(testLabelE,y_te_pred2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}